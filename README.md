# Massive YouTube Educational Transcription

Autonomous pipeline for transcribing YouTube educational content at scale. Uses faster-whisper on multi-GPU to produce a large open educational text dataset.

## Current Stats

| Metric | Value |
|--------|-------|
| **Transcribed** | 86,138 videos (~60,742 audio hours) |
| **Characters** | 2.58B (~644M tokens) |
| **Queue** | 5.39M pending (5.65M total discovered) |
| **Speed** | 165â€“185Ã— realtime per GPU |
| **Throughput** | ~550 videos/hr (4 GPUs) |

## Datasets

| Dataset | Description |
|---------|-------------|
| [massive-yt-edu-transcriptions](https://huggingface.co/datasets/thepowerfuldeez/massive-yt-edu-transcriptions) | Full transcripts (daily auto-push) |
| [massive-yt-edu-queue](https://huggingface.co/datasets/thepowerfuldeez/massive-yt-edu-queue) | 4.5M video metadata with content classification + license risk |

## Architecture

```
SQLite DB (WAL mode) â† single source of truth
â”œâ”€â”€ GPU Workers (4Ã—)
â”‚   â”œâ”€â”€ 2 prefetch threads each (yt-dlp + ffmpeg 1.2Ã— atempo â†’ mp3)
â”‚   â”œâ”€â”€ Cookie pool rotation (5 accounts, per-thread copies)
â”‚   â””â”€â”€ faster-whisper CTranslate2 (distil-large-v3.5, beam=1, no VAD)
â”œâ”€â”€ Discovery Crawlers
â”‚   â”œâ”€â”€ Channel crawler (full catalog extraction, snowball via related)
â”‚   â”œâ”€â”€ Related video walker (playlist + recommendation chains)
â”‚   â””â”€â”€ CC-focused discovery (known OCW channels + CC search filters)
â”œâ”€â”€ License Scanner
â”‚   â”œâ”€â”€ yt-dlp description + license field fetcher
â”‚   â””â”€â”€ YouTube Data API v3 batch scanner (50 IDs/request)
â””â”€â”€ HuggingFace Export (daily cron)
```

### Key Design Decisions

- **faster-whisper over HF pipeline**: 3.3Ã— faster, 2.5GB VRAM vs 6â€“8GB (CTranslate2 fused kernels)
- **1.2Ã— audio speedup**: yt-dlp atempo filter â€” 17% less GPU work, negligible quality loss
- **No VAD**: Silero VAD benchmarked â€” adds overhead on dense educational lectures
- **beam_size=1**: Max throughput for batch workload
- **SQLite as queue**: Atomic `UPDATE...RETURNING` claims, WAL mode, no external queue service
- **Cookie pool**: YouTube blocks unauthenticated bulk downloads; 5 accounts rotate per-thread
- **Process group kill**: `start_new_session=True` + `os.killpg()` prevents zombie yt-dlp/ffmpeg
- **Post-processing skipped**: Whisper large-v3 already produces properly punctuated, capitalized text

## Content Classification

Every video is classified by content source and license risk:

| Risk | Count | Description |
|------|-------|-------------|
| ğŸŸ¢ Green | 129K | CC-licensed or public domain (NPTEL, Khan, MIT OCW, Yale OYC, Taiwan OCW) |
| ğŸŸ¡ Yellow | 3.99M | Standard YouTube license, fair use for research |
| ğŸŸ  Orange | 300K | Commercial/copyrighted, needs review |
| ğŸ”´ Red | 72K | Non-educational (gaming, music, vlogs) â€” excluded from transcription |

### Classification Method

1. **Channel/source name matching** â€” 207K channels classified via pattern matching (universities, conferences, govt agencies, etc.)
2. **Title analysis** â€” regex for course codes, "Lecture N", conference names, gaming terms
3. **Priority fallback** â€” P8+ videos assumed educational
4. **CC verification** â€” YouTube license field + description text mining + publisher website policy checks

### Known CC Sources (~72K videos)

- NPTEL/IIT (~39K) â€” CC-BY-SA 4.0 (Indian govt funded)
- Taiwan OCW: NTHU + NYCU (~15K) â€” CC-BY-NC-SA
- Khan Academy (~8.5K) â€” CC-BY-NC-SA 3.0
- Library of Congress (~5.3K) â€” Public domain
- MIT OCW â€” CC-BY-NC-SA 4.0 (verified from website)
- Yale OYC â€” CC-BY-NC-SA 3.0 (verified from website)

## Quality Filter

Videos must pass a two-stage filter:

1. **Duration**: â‰¥15 minutes (deep educational content only)
2. **Content**: 40+ reject categories (gaming, music, vlogs, ASMR, pranks, religious sermons, conspiracy, etc.)
3. **Priority boost**: P9 for university courses/conferences, P8 for lectures/edu creators, P7 for docs/explainers

## Hardware

| GPU | Model | VRAM | Avg Speed |
|-----|-------|------|-----------|
| 0 | RTX 5090 | 32GB | ~179Ã— |
| 1 | RTX 5090 | 32GB | ~183Ã— |
| 2 | RTX 4090 | 24GB | ~226Ã— |
| 3 | RTX 4090 | 24GB | ~183Ã— |

~2.5GB VRAM per GPU. Rest available for other workloads.

## Quick Start

```bash
pip install faster-whisper librosa numpy huggingface_hub
mkdir -p ~/academic_transcriptions/{tmp_gpu{0,1,2,3},cookie_pool}

# Place Netscape-format cookie files in cookie_pool/
# Place yt-dlp binary in ~/academic_transcriptions/

bash launch.sh              # Start 4 GPU workers
bash launch_discovery.sh    # Start discovery crawlers
python3 src/export_hf.py    # Export + push to HuggingFace
```

## Files

```
â”œâ”€â”€ README.md
â”œâ”€â”€ LICENSING_ANALYSIS.md       # Full licensing report with outreach strategy
â”œâ”€â”€ POSTPROCESSING_RESEARCH.md  # Why we skip post-processing
â”œâ”€â”€ launch.sh                   # GPU worker launcher
â”œâ”€â”€ launch_discovery.sh         # Discovery crawler launcher
â”œâ”€â”€ watchdog.sh                 # Health check (runs via cron)
â””â”€â”€ src/
    â”œâ”€â”€ worker.py               # GPU transcription worker
    â”œâ”€â”€ quality_filter.py       # Content quality/reject patterns
    â”œâ”€â”€ discover_related.py     # Related video + playlist discovery
    â”œâ”€â”€ discover_channels_10M.py # Channel-based bulk discovery
    â”œâ”€â”€ discover_safe.py        # CC-focused safe content discovery
    â”œâ”€â”€ discover_cc.py          # CC content chain discovery
    â”œâ”€â”€ fetch_descriptions.py   # Batch description + license fetcher
    â”œâ”€â”€ batch_license_scan.py   # YouTube Data API license scanner
    â”œâ”€â”€ export_hf.py            # Transcription dataset export
    â”œâ”€â”€ export_queue_hf.py      # Queue metadata export
    â””â”€â”€ monitor.py              # Real-time progress monitor
```

## Fair Use Analysis

This dataset is built under fair use for ML research:

- **Transformative**: Audio â†’ text, different medium and purpose
- **Factual content**: Educational lectures are factual, not creative works
- **No market substitution**: Text transcripts don't replace video lectures
- **Research purpose**: Dataset for training/evaluating language models

See [LICENSING_ANALYSIS.md](LICENSING_ANALYSIS.md) for the full legal framework and outreach strategy.

## License

MIT
